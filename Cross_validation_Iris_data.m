Iris=csvread("G:/MS Courses/Fall 2016/Intro to Machine learning/homework 2/iris.data.csv");% transforms the entire dataset to have input values between 0 and 1% assumes the last column is the output values and ignores it.% use this function if you wish. function ret = normalize_dataset(dataset)   ret = dataset;  for i=1:columns(dataset)-1    smallest = min(dataset(:,i));    largest = max(dataset(:,i));    for j=1:rows(dataset)      ret(j,i) = (dataset(j,i) - smallest)/(largest-smallest);    endfor  endforendfunction% determines the average misclassification rate (between 0 and 1)function ret = classification_error(inputs, outputs, classifier)  ret = 0;  for i=1:rows(inputs)    if(classifier(inputs(i,:)) != outputs(i))      ret = ret + 1;    endif  endfor  ret = ret / rows(inputs);endfunction% performs cross validation with the indicated dataset and number of iterations% for instance, iterations=10 performs 10-fold cross validation% trainer: a function that takes a matrix of inputs and a vector of corresponding%   outputs and returns a funtion which classifies an input vector x, based on %   the training data. See below for a trainer generator for knnfunction ret = cross_validate(dataset, iterations, trainer)  ret = 0;  testsize = floor(rows(dataset)/iterations);  trainingsize = rows(dataset) - testsize;  fullsize = rows(dataset);  for i=0:iterations-1    if i==0      training = dataset(1:trainingsize,:);      testing = dataset(trainingsize+1:trainingsize+testsize,:);    elseif i==iterations-1      training = dataset(testsize+1:fullsize,:);      testing = dataset(1:testsize,:);    else      training = dataset([1:i*testsize,(i+1)*testsize+1:fullsize],:);      testing = dataset(i*testsize+1:(i+1)*testsize,:);    endif    classifier = trainer(training(:,1:columns(training)-1), training(:,columns(training)));    ret = ret + classification_error(testing(:,1:columns(training)-1), testing(:,columns(training)), classifier);  endfor  ret = ret / iterations;endfunction% the knn classification algorithm, altered to use a distance and voting functionfunction ret = knn_classifier(x, inputs, outputs, k, dist, vote)  distances = inf * ones(1,k);  targets = zeros(1,k);    % find k nearest neighbors  for i=1:length(inputs)    d = dist(x, inputs(i,:));    if d < distances(1)       j = 1;      % shift lift of nearest neighbors      while j < k && d < distances(j+1);        distances(j) = distances(j+1);        targets(j) = targets(j+1);        j = j + 1;      endwhile      distances(j) = d;      targets(j) = outputs(i);    endif  endfor  ret = vote(targets, distances);endfunction% basic distance functionfunction ret = euclidean_dist(x1, x2)   ret = sum((x1-x2).^2);endfunction% basic voting functionfunction ret = majority_vote(outputs, distances)  ret = mode(outputs);endfunction% my distance functionfunction ret = weighted_dist(x1, x2)   ret = sum((x1-x2).^4);endfunction% my voting functionfunction ret = weighted_vote(outputs, distances)  ret = median(outputs);endfunction% the code below uses the default distance/voting functions and k=1% to perform 10-fold cross validation. This is a starting point for % your analysis. Follow the instructions in the homework and develop % a new distance and/or voting function with the goal of improving % the results. Compare your results with the default methods.  nIris = normalize_dataset(Iris); %normalize the Iris datasetnrIris = nIris(randperm(rows(Iris)), :); %randomize the rows in the datasetk=1 % k parameter for k-NNtrainer = @(tx, ty) @(x) knn_classifier(x, tx, ty, k, @euclidean_dist, @majority_vote);accuracy = 1-cross_validate(nrIris, 45, trainer)%nIris = normalize_dataset(Iris); %normalize the Iris dataset%nrIris = nIris(randperm(rows(Iris)), :); %randomize the rows in the dataset%k=1 % k parameter for k-NN%trainer = @(tx, ty) @(x) knn_classifier(x, tx, ty, k, @weighted_dist, @weighted_vote);%accuracy = 1-cross_validate(nrIris, 45, trainer)%START:cross_validate with at least 5 substantially different values of%k values combined with 5 different training set sizes.%nIris = normalize_dataset(Iris); %normalize the Iris dataset%nrIris = nIris(randperm(rows(Iris)), :); %randomize the rows in the dataset%k=3 % k parameter for k-NN%trainer = @(tx, ty) @(x) knn_classifier(x, tx, ty, k, @euclidean_dist, @majority_vote);%accuracy = 1-cross_validate(nrIris, 15, trainer)%nIris = normalize_dataset(Iris); %normalize the Iris dataset%nrIris = nIris(randperm(rows(Iris)), :); %randomize the rows in the dataset%k=5 % k parameter for k-NN%trainer = @(tx, ty) @(x) knn_classifier(x, tx, ty, k, @euclidean_dist, @majority_vote);%accuracy = 1-cross_validate(nrIris, 45, trainer)%nIris = normalize_dataset(Iris); %normalize the Iris dataset%nrIris = nIris(randperm(rows(Iris)), :); %randomize the rows in the dataset%k=7 % k parameter for k-NN%trainer = @(tx, ty) @(x) knn_classifier(x, tx, ty, k, @euclidean_dist, @majority_vote);%accuracy = 1-cross_validate(nrIris, 75, trainer)%nIris = normalize_dataset(Iris); %normalize the Iris dataset%nrIris = nIris(randperm(rows(Iris)), :); %randomize the rows in the dataset%k=9 % k parameter for k-NN%trainer = @(tx, ty) @(x) knn_classifier(x, tx, ty, k, @euclidean_dist, @majority_vote);%accuracy = 1-cross_validate(nrIris, 90, trainer)%END:cross_validate with at least 5 substantially different values of%k values combined with 5 different training set sizes.